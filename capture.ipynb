{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2c7c497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           State Action                      NextState\n",
      "0  (4378.5, 30.0, 4378.75, 24.0)   WAIT  (4378.5, 30.0, 4378.75, 23.0)\n",
      "1  (4378.5, 30.0, 4378.75, 23.0)   WAIT  (4378.5, 31.0, 4378.75, 23.0)\n",
      "2  (4378.5, 31.0, 4378.75, 23.0)   WAIT  (4378.5, 32.0, 4378.75, 23.0)\n",
      "3  (4378.5, 32.0, 4378.75, 23.0)   WAIT  (4378.5, 32.0, 4378.75, 22.0)\n",
      "4  (4378.5, 32.0, 4378.75, 22.0)   WAIT  (4378.5, 32.0, 4378.75, 19.0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('ESc1_filtered.csv')\n",
    "\n",
    "# Assume these columns: Time, Type, BidPrice, BidSize, AskPrice, AskSize\n",
    "# Adjust the column names based on your file!\n",
    "\n",
    "# Initialize\n",
    "states = []\n",
    "actions = []\n",
    "next_states = []\n",
    "\n",
    "# Helper function to construct a state\n",
    "def construct_state(row):\n",
    "    return (row['Bid Price'], row['Bid Size'], row['Ask Price'], row['Ask Size'])\n",
    "\n",
    "# Iterate over the rows\n",
    "for i in range(len(df) - 1):\n",
    "    current_row = df.iloc[i]\n",
    "    next_row = df.iloc[i+1]\n",
    "\n",
    "    # Build current state\n",
    "    state = construct_state(current_row)\n",
    "\n",
    "    # Infer action based on \"Type\" and Trade Price vs Bid/Ask\n",
    "    if current_row['Type'] == 'Trade':\n",
    "        trade_price = current_row['Bid Price']  # or get the TradePrice column if separate\n",
    "        \n",
    "        if abs(trade_price - current_row['Ask Price']) < 1e-4:\n",
    "            action = 'BUY'\n",
    "        elif abs(trade_price - current_row['Bid Price']) < 1e-4:\n",
    "            action = 'SELL'\n",
    "        else:\n",
    "            action = 'WAIT'\n",
    "    else:\n",
    "        action = 'WAIT'  # If Quote update only → Assume WAIT\n",
    "\n",
    "    # Build next state\n",
    "    next_state = construct_state(next_row)\n",
    "\n",
    "    # Save to lists\n",
    "    states.append(state)\n",
    "    actions.append(action)\n",
    "    next_states.append(next_state)\n",
    "\n",
    "# Now you have the full MDP dataset\n",
    "mdp = pd.DataFrame({\n",
    "    'State': states,\n",
    "    'Action': actions,\n",
    "    'NextState': next_states\n",
    "})\n",
    "\n",
    "print(mdp.head())\n",
    "\n",
    "# Save for future use\n",
    "mdp.to_csv('mdp_orderbook.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aed69d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LNIRL Learned Weights: [0.44297942 0.12988212 0.04461005 0.62721423 0.48056566 0.40071034]\n",
      "GPIRL Learned Weights shape: (33870,)\n",
      "SVM Accuracy with LNIRL rewards: 0.9426\n",
      "SVM Accuracy with GPIRL rewards: 0.9426\n",
      "Evaluation round 1/5\n",
      "  Baseline: 0.9426, LNIRL: 0.9426, GPIRL: 0.9426\n",
      "Evaluation round 2/5\n",
      "  Baseline: 0.9424, LNIRL: 0.9424, GPIRL: 0.9424\n",
      "Evaluation round 3/5\n",
      "  Baseline: 0.9441, LNIRL: 0.9441, GPIRL: 0.9441\n",
      "Evaluation round 4/5\n",
      "  Baseline: 0.9428, LNIRL: 0.9428, GPIRL: 0.9428\n",
      "Evaluation round 5/5\n",
      "  Baseline: 0.9436, LNIRL: 0.9436, GPIRL: 0.9436\n",
      "\n",
      "Average Results:\n",
      "Baseline: 0.9431 ± 0.0007\n",
      "LNIRL: 0.9431 ± 0.0007\n",
      "GPIRL: 0.9431 ± 0.0007\n",
      "Analysis complete! Results saved to 'irl_comparison_results.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_556755/2403632930.py:313: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  plt.boxplot([results['baseline'], results['lnirl'], results['gpirl']],\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "import re\n",
    "\n",
    "# Load the processed MDP data\n",
    "mdp = pd.read_csv('mdp_orderbook.csv')\n",
    "\n",
    "# Convert state tuples from string representation to actual tuples\n",
    "def parse_state(state_str):\n",
    "    # Remove outer parentheses and split by comma\n",
    "    values = state_str.strip('()').split(',')\n",
    "    # Convert to appropriate numeric types\n",
    "    parsed_values = []\n",
    "    \n",
    "    for v in values:\n",
    "        v = v.strip()\n",
    "        # Check if value contains numpy function call\n",
    "        if 'np.' in v:\n",
    "            # Extract just the numeric value, properly handling parentheses\n",
    "            match = re.search(r'\\((.*?)(\\)|$)', v)  # Modified regex to handle closing parenthesis\n",
    "            if match:\n",
    "                num_str = match.group(1).strip()\n",
    "                parsed_values.append(float(num_str))\n",
    "            else:\n",
    "                # If regex doesn't match, try a different approach\n",
    "                try:\n",
    "                    # Remove any non-numeric chars except decimal point and negative sign\n",
    "                    clean_v = re.sub(r'[^\\d.-]', '', v)\n",
    "                    parsed_values.append(float(clean_v))\n",
    "                except ValueError:\n",
    "                    print(f\"Warning: Could not parse value: {v}\")\n",
    "                    parsed_values.append(np.nan)\n",
    "        else:\n",
    "            try:\n",
    "                parsed_values.append(float(v))\n",
    "            except ValueError:\n",
    "                # Handle potential extra parentheses or other characters\n",
    "                clean_v = re.sub(r'[^\\d.-]', '', v)\n",
    "                try:\n",
    "                    parsed_values.append(float(clean_v))\n",
    "                except ValueError:\n",
    "                    print(f\"Warning: Could not parse value: {v}\")\n",
    "                    parsed_values.append(np.nan)\n",
    "    \n",
    "    return tuple(parsed_values)\n",
    "    \n",
    "    return tuple(parsed_values)\n",
    "# Apply parsing if states are stored as strings\n",
    "if isinstance(mdp['State'].iloc[0], str):\n",
    "    mdp['State'] = mdp['State'].apply(parse_state)\n",
    "    mdp['NextState'] = mdp['NextState'].apply(parse_state)\n",
    "\n",
    "# Convert states to feature vectors for ML algorithms\n",
    "def state_to_features(state):\n",
    "    # Extract bid-ask spread, sizes, and other features\n",
    "    bid_price, bid_size, ask_price, ask_size = state\n",
    "    spread = ask_price - bid_price\n",
    "    imbalance = bid_size / (bid_size + ask_size) if (bid_size + ask_size) > 0 else 0.5\n",
    "    \n",
    "    return np.array([bid_price, bid_size, ask_price, ask_size, spread, imbalance])\n",
    "\n",
    "# Prepare data\n",
    "X = np.vstack([state_to_features(state) for state in mdp['State']])\n",
    "y = mdp['Action']\n",
    "\n",
    "# Encode actions as integers\n",
    "action_map = {'WAIT': 0, 'BUY': 1, 'SELL': 2}\n",
    "y_encoded = np.array([action_map[a] for a in y])\n",
    "\n",
    "# Split data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ====== LNIRL Implementation ======\n",
    "def linear_irl(states, actions, gamma=0.9, learning_rate=0.01, iterations=1000):\n",
    "    \"\"\"\n",
    "    Linear IRL implementation to recover the reward function\n",
    "    \n",
    "    Parameters:\n",
    "    states - array of state features\n",
    "    actions - array of corresponding actions\n",
    "    gamma - discount factor\n",
    "    learning_rate - step size for gradient ascent\n",
    "    iterations - number of iterations for optimization\n",
    "    \n",
    "    Returns:\n",
    "    weights - learned feature weights for the reward function\n",
    "    \"\"\"\n",
    "    num_features = states.shape[1]\n",
    "    weights = np.random.rand(num_features)\n",
    "    \n",
    "    # Get unique actions\n",
    "    unique_actions = np.unique(actions)\n",
    "    num_actions = len(unique_actions)\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        # Calculate rewards\n",
    "        rewards = np.dot(states, weights)\n",
    "        \n",
    "        # Compute gradients for each action\n",
    "        gradients = np.zeros_like(weights)\n",
    "        \n",
    "        for action in unique_actions:\n",
    "            # Expert demonstrations for this action\n",
    "            expert_states = states[actions == action]\n",
    "            \n",
    "            if len(expert_states) > 0:\n",
    "                # Compute expert feature expectations\n",
    "                expert_features = np.mean(expert_states, axis=0)\n",
    "                \n",
    "                # Compute features for all states where this action was taken\n",
    "                action_probability = len(expert_states) / len(states)\n",
    "                \n",
    "                # Update gradient\n",
    "                gradients += action_probability * (expert_features - np.mean(states, axis=0))\n",
    "        \n",
    "        # Update weights\n",
    "        weights += learning_rate * gradients\n",
    "        \n",
    "        # Normalize weights\n",
    "        weights = weights / np.linalg.norm(weights)\n",
    "        \n",
    "    return weights\n",
    "\n",
    "# Apply LNIRL to learn reward weights\n",
    "lnirl_weights = linear_irl(X_train_scaled, y_train)\n",
    "print(\"LNIRL Learned Weights:\", lnirl_weights)\n",
    "\n",
    "# Calculate rewards using learned weights\n",
    "lnirl_rewards = np.dot(X_test_scaled, lnirl_weights)\n",
    "\n",
    "# ====== GPIRL Implementation ======\n",
    "def gaussian_process_irl(states, actions, gamma=0.9, sigma=1.0, iterations=100):\n",
    "    \"\"\"\n",
    "    Gaussian Process IRL implementation\n",
    "    \n",
    "    Parameters:\n",
    "    states - array of state features\n",
    "    actions - array of corresponding actions\n",
    "    gamma - discount factor\n",
    "    sigma - kernel bandwidth parameter\n",
    "    iterations - number of iterations\n",
    "    \n",
    "    Returns:\n",
    "    alpha - GP weights\n",
    "    \"\"\"\n",
    "    n_samples = states.shape[0]\n",
    "    \n",
    "    # Compute kernel matrix\n",
    "    kernel = RBF(length_scale=sigma)\n",
    "    K = kernel(states)\n",
    "    \n",
    "    # Initialize alpha (GP weights)\n",
    "    alpha = np.zeros(n_samples)\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        # Compute rewards\n",
    "        rewards = K @ alpha\n",
    "        \n",
    "        # Compute gradient of max likelihood\n",
    "        gradient = np.zeros(n_samples)\n",
    "        \n",
    "        # For each action, compute the likelihood gradient\n",
    "        unique_actions = np.unique(actions)\n",
    "        for action in unique_actions:\n",
    "            action_mask = actions == action\n",
    "            if np.any(action_mask):\n",
    "                # Expert feature expectations\n",
    "                expert_indices = np.where(action_mask)[0]\n",
    "                non_expert_indices = np.where(~action_mask)[0]\n",
    "                \n",
    "                if len(expert_indices) > 0 and len(non_expert_indices) > 0:\n",
    "                    # Compute likelihood ratio gradient\n",
    "                    expert_rewards = rewards[expert_indices]\n",
    "                    non_expert_rewards = rewards[non_expert_indices]\n",
    "                    \n",
    "                    # Calculate softmax probabilities\n",
    "                    max_reward = np.max(rewards)\n",
    "                    exp_rewards = np.exp(rewards - max_reward)\n",
    "                    Z = np.sum(exp_rewards)\n",
    "                    \n",
    "                    # Update gradient\n",
    "                    for idx in expert_indices:\n",
    "                        gradient[idx] += 1.0 / len(expert_indices) - exp_rewards[idx] / Z\n",
    "        \n",
    "        # Update alpha\n",
    "        alpha += 0.01 * gradient\n",
    "        \n",
    "    return alpha\n",
    "\n",
    "# Apply GPIRL\n",
    "gpirl_weights = gaussian_process_irl(X_train_scaled, y_train)\n",
    "print(\"GPIRL Learned Weights shape:\", gpirl_weights.shape)\n",
    "\n",
    "# Compute GP rewards for test set\n",
    "kernel = RBF(length_scale=1.0)\n",
    "K_test = kernel(X_test_scaled, X_train_scaled)\n",
    "gpirl_rewards = K_test @ gpirl_weights\n",
    "\n",
    "# ====== SVM Training on Recovered Rewards ======\n",
    "# Combine original features with recovered rewards\n",
    "X_train_with_rewards = np.column_stack([X_train_scaled, np.dot(X_train_scaled, lnirl_weights)])\n",
    "X_test_with_rewards = np.column_stack([X_test_scaled, np.dot(X_test_scaled, lnirl_weights)])\n",
    "\n",
    "# Train SVM\n",
    "svm = SVC(kernel='rbf', C=10.0)\n",
    "svm.fit(X_train_with_rewards, y_train)\n",
    "\n",
    "# Evaluate accuracy\n",
    "y_pred = svm.predict(X_test_with_rewards)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"SVM Accuracy with LNIRL rewards: {accuracy:.4f}\")\n",
    "\n",
    "# Try with GPIRL rewards\n",
    "# Compute GP rewards for training set\n",
    "K_train = kernel(X_train_scaled, X_train_scaled)\n",
    "gpirl_train_rewards = K_train @ gpirl_weights\n",
    "\n",
    "# Combine features with GP rewards\n",
    "X_train_with_gp_rewards = np.column_stack([X_train_scaled, gpirl_train_rewards])\n",
    "X_test_with_gp_rewards = np.column_stack([X_test_scaled, gpirl_rewards])\n",
    "\n",
    "# Train SVM with GP rewards\n",
    "svm_gp = SVC(kernel='rbf', C=10.0)\n",
    "svm_gp.fit(X_train_with_gp_rewards, y_train)\n",
    "\n",
    "# Evaluate accuracy\n",
    "y_pred_gp = svm_gp.predict(X_test_with_gp_rewards)\n",
    "accuracy_gp = accuracy_score(y_test, y_pred_gp)\n",
    "print(f\"SVM Accuracy with GPIRL rewards: {accuracy_gp:.4f}\")\n",
    "\n",
    "# ====== Multiple Sampling Rounds Evaluation ======\n",
    "def evaluate_multiple_rounds(X, y, n_rounds=5, test_size=0.3):\n",
    "    lnirl_accuracies = []\n",
    "    gpirl_accuracies = []\n",
    "    baseline_accuracies = []\n",
    "    \n",
    "    for i in range(n_rounds):\n",
    "        print(f\"Evaluation round {i+1}/{n_rounds}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=i*10)\n",
    "        \n",
    "        # Standardize\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # LNIRL\n",
    "        lnirl_weights = linear_irl(X_train_scaled, y_train, iterations=500)\n",
    "        X_train_lnirl = np.column_stack([X_train_scaled, np.dot(X_train_scaled, lnirl_weights)])\n",
    "        X_test_lnirl = np.column_stack([X_test_scaled, np.dot(X_test_scaled, lnirl_weights)])\n",
    "        \n",
    "        # GPIRL\n",
    "        gpirl_weights = gaussian_process_irl(X_train_scaled, y_train, iterations=50)\n",
    "        kernel = RBF(length_scale=1.0)\n",
    "        K_train = kernel(X_train_scaled, X_train_scaled)\n",
    "        K_test = kernel(X_test_scaled, X_train_scaled)\n",
    "        gpirl_train_rewards = K_train @ gpirl_weights\n",
    "        gpirl_test_rewards = K_test @ gpirl_weights\n",
    "        X_train_gpirl = np.column_stack([X_train_scaled, gpirl_train_rewards])\n",
    "        X_test_gpirl = np.column_stack([X_test_scaled, gpirl_test_rewards])\n",
    "        \n",
    "        # Train models\n",
    "        svm_baseline = SVC(kernel='rbf', C=10.0)\n",
    "        svm_baseline.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        svm_lnirl = SVC(kernel='rbf', C=10.0)\n",
    "        svm_lnirl.fit(X_train_lnirl, y_train)\n",
    "        \n",
    "        svm_gpirl = SVC(kernel='rbf', C=10.0)\n",
    "        svm_gpirl.fit(X_train_gpirl, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        baseline_acc = accuracy_score(y_test, svm_baseline.predict(X_test_scaled))\n",
    "        lnirl_acc = accuracy_score(y_test, svm_lnirl.predict(X_test_lnirl))\n",
    "        gpirl_acc = accuracy_score(y_test, svm_gpirl.predict(X_test_gpirl))\n",
    "        \n",
    "        baseline_accuracies.append(baseline_acc)\n",
    "        lnirl_accuracies.append(lnirl_acc)\n",
    "        gpirl_accuracies.append(gpirl_acc)\n",
    "        \n",
    "        print(f\"  Baseline: {baseline_acc:.4f}, LNIRL: {lnirl_acc:.4f}, GPIRL: {gpirl_acc:.4f}\")\n",
    "    \n",
    "    # Average results\n",
    "    print(\"\\nAverage Results:\")\n",
    "    print(f\"Baseline: {np.mean(baseline_accuracies):.4f} ± {np.std(baseline_accuracies):.4f}\")\n",
    "    print(f\"LNIRL: {np.mean(lnirl_accuracies):.4f} ± {np.std(lnirl_accuracies):.4f}\")\n",
    "    print(f\"GPIRL: {np.mean(gpirl_accuracies):.4f} ± {np.std(gpirl_accuracies):.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'baseline': baseline_accuracies,\n",
    "        'lnirl': lnirl_accuracies,\n",
    "        'gpirl': gpirl_accuracies\n",
    "    }\n",
    "\n",
    "# Run multiple evaluation rounds\n",
    "results = evaluate_multiple_rounds(X, y_encoded, n_rounds=5)\n",
    "\n",
    "# Visualize results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot([results['baseline'], results['lnirl'], results['gpirl']], \n",
    "            labels=['Baseline', 'LNIRL', 'GPIRL'])\n",
    "plt.title('Classification Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.savefig('irl_comparison_results.png')\n",
    "plt.close()\n",
    "\n",
    "print(\"Analysis complete! Results saved to 'irl_comparison_results.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afc19e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
